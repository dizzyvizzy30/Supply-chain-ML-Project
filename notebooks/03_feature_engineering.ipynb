{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom category_encoders import TargetEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12,6)\n\nprint(\"Libraries imported successfully!\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Interaction features\n# Vendor-Country combination (encode as categorical)\ndf['vendor_country_combo'] = df['Vendor'].astype(str) + '_' + df['Country'].astype(str)\n\n# Shipment mode-Country combination\ndf['mode_country_combo'] = df['Shipment Mode'].astype(str) + '_' + df['Country'].astype(str)\n\n# Vendor-Mode combination\ndf['vendor_mode_combo'] = df['Vendor'].astype(str) + '_' + df['Shipment Mode'].astype(str)\n\n\nprint(\"Interaction features created:\")\nprint(f\"- vendor_country_combo: {df['vendor_country_combo'].nunique()} unique values\")\nprint(f\"- mode_country_combo: {df['mode_country_combo'].nunique()} unique values\")\nprint(f\"- vendor_mode_combo: {df['vendor_mode_combo'].nunique()} unique values\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interaction features\n",
    "# Vendor-Country combination (encode as categorical)\n",
    "df['vendor_country_combo'] = df['Vendor'].astype(str) + '_' + df['Country'].astype(str)\n",
    "\n",
    "# Shipment mode-Country combination\n",
    "df['mode_country_combo'] = df['Shipment Mode'].astype(str) + '_' + df['Country'].astype(str)\n",
    "\n",
    "# Vendor-Mode combination\n",
    "df['vendor_mode_combo'] = df['Vendor'].astype(str) + '_' + df['Shipment Mode'].astype(str)\n",
    "\n",
    "print(\"Interaction features created:\")\n",
    "print(f\"- vendor_country_combo: {df['vendor_country_combo'].nunique()} unique values\")\n",
    "print(f\"- mode_country_combo: {df['mode_country_combo'].nunique()} unique values\")\n",
    "print(f\"- vendor_mode_combo: {df['vendor_mode_combo'].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ratio features\n",
    "# Create weight to cost ratio (efficiency metric)\n",
    "df['weight_to_cost_ratio'] = df['Weight (Kilograms)'] / (df['Freight Cost (USD)'] + 1)  # +1 to avoid division by zero\n",
    "\n",
    "# Value density (value per weight)\n",
    "df['value_density'] = df['Line Item Value'] / (df['Weight (Kilograms)'] + 0.1)\n",
    "\n",
    "# Lead time to delivery ratio\n",
    "df['lead_time_ratio'] = df['lead_time_days'] / (df['delay_days'].abs() + 1)\n",
    "\n",
    "print(\"\\nRatio features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Boolean features\n",
    "# High value shipment indicator\n",
    "df['is_high_value'] = (df['Line Item Value'] > df['Line Item Value'].quantile(0.75)).astype(int)\n",
    "\n",
    "# Heavy shipment indicator\n",
    "df['is_heavy'] = (df['Weight (Kilograms)'] > df['Weight (Kilograms)'].quantile(0.75)).astype(int)\n",
    "\n",
    "# Urgent shipment (air mode)\n",
    "df['is_air_shipment'] = (df['Shipment Mode'] == 'Air').astype(int)\n",
    "\n",
    "# Long lead time indicator\n",
    "df['is_long_lead_time'] = (df['lead_time_days'] > 30).astype(int)\n",
    "\n",
    "print(\"\\nBoolean features created:\")\n",
    "print(f\"- High value shipments: {df['is_high_value'].sum():,}\")\n",
    "print(f\"- Heavy shipments: {df['is_heavy'].sum():,}\")\n",
    "print(f\"- Air shipments: {df['is_air_shipment'].sum():,}\")\n",
    "print(f\"- Long lead time: {df['is_long_lead_time'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Strategy for handling missing values\n# For numerical columns: impute with median\n# For categorical columns: create 'Unknown' category\n\nnumerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n\n# Remove target and ID columns from imputation\nnumerical_cols = [col for col in numerical_cols if col not in ['is_late', 'delay_days', 'ID']]\n\n# Impute numerical columns with median\nfor col in numerical_cols:\n    if df[col].isnull().sum() > 0:\n        median_val = df[col].median()\n        df[col].fillna(median_val, inplace=True)\n        print(f\"Imputed {col} with median: {median_val:.2f}\")\n\n# Impute categorical columns with 'Unknown'\nfor col in categorical_cols:\n    if df[col].isnull().sum() > 0:\n        df[col].fillna('Unknown', inplace=True)\n        print(f\"Imputed {col} with 'Unknown'\")\n\n\nprint(f\"\\nMissing values after imputation: {df.isnull().sum().sum()}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\"*60)\n",
    "if len(missing_summary) > 0:\n",
    "    print(missing_summary.head(20))\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Strategy for handling missing values\n",
    "# For numerical columns: impute with median\n",
    "# For categorical columns: create 'Unknown' category\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns from imputation\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['is_late', 'delay_days', 'ID']]\n",
    "\n",
    "# Impute numerical columns with median\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Imputed {col} with median: {median_val:.2f}\")\n",
    "\n",
    "# Impute categorical columns with 'Unknown'\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna('Unknown', inplace=True)\n",
    "        print(f\"Imputed {col} with 'Unknown'\")\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection - Identify Relevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define features to use for ML\n",
    "# Exclude: ID columns, target, delay_days, date columns, raw text fields\n",
    "\n",
    "exclude_cols = [\n",
    "    'ID', 'is_late', 'delay_days',\n",
    "    'PQ First Sent to Client Date', 'PO Sent to Vendor Date',\n",
    "    'Scheduled Delivery Date', 'Delivered to Client Date', 'Delivery Recorded Date',\n",
    "    'PQ #', 'PO / SO #', 'ASN/DN #',\n",
    "    'Item Description', 'Molecule/Test Type', 'Brand',  # High cardinality text\n",
    "    'delivery_year_month'  # Period object\n",
    "]\n",
    "\n",
    "# Get all columns\n",
    "all_cols = df.columns.tolist()\n",
    "\n",
    "# Feature columns (exclude target and non-predictive)\n",
    "feature_cols = [col for col in all_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total columns: {len(all_cols)}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "print(f\"Excluded columns: {len(exclude_cols)}\")\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = [col for col in feature_cols if df[col].dtype in [np.int64, np.float64]]\n",
    "categorical_features = [col for col in feature_cols if df[col].dtype == 'object']\n",
    "\n",
    "print(f\"\\nNumerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Calculate correlation with target\n# Add target back temporarily for correlation analysis\nX_train_with_target = X_train.copy()\nX_train_with_target['is_late'] = y_train.values\n\n# Compute correlations with target\ncorrelations = X_train_with_target.corr()['is_late'].drop('is_late').sort_values(ascending=False)\n\nprint(\"Top 20 Features Correlated with Delay:\")\nprint(\"=\"*60)\nprint(correlations.head(20))\n\nprint(\"\\nTop 20 Features Negatively Correlated with Delay:\")\nprint(\"=\"*60)\nprint(correlations.tail(20))",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Visualze top correlations (minor typo)\nplt.figure(figsize=(10, 8))\n\n# Top 15 positive and negative correlations\ntop_corr = pd.concat([correlations.head(15), correlations.tail(15)])\n\nplt.barh(range(len(top_corr)), top_corr.values)\nplt.yticks(range(len(top_corr)), top_corr.index, fontsize=8)\nplt.xlabel('Correlation with Delay')\nplt.title('Top Features Correlated with Delivery Delays')\nplt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# One-hot encode low cardinality columns\n",
    "if len(low_cardinality_cols) > 0:\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=low_cardinality_cols, prefix=low_cardinality_cols, drop_first=True)\n",
    "    print(f\"One-hot encoding applied to {len(low_cardinality_cols)} columns\")\n",
    "\n",
    "print(f\"Shape after one-hot encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Target encode high cardinality columns\n",
    "# Note: In production, fit on train set only to avoid leakage\n",
    "# Here we'll do a simple train-test split first\n",
    "\n",
    "# Create train/test split\n",
    "train_df, test_df = train_test_split(df_encoded, test_size=0.2, random_state=42, stratify=df_encoded['is_late'])\n",
    "\n",
    "print(f\"Train set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print(f\"\\nTrain delay rate: {train_df['is_late'].mean():.2%}\")\n",
    "print(f\"Test delay rate: {test_df['is_late'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply target encoding to high cardinality columns\n",
    "if len(high_cardinality_cols) > 0:\n",
    "    # Fit target encoder on training data\n",
    "    target_encoder = TargetEncoder(cols=high_cardinality_cols)\n",
    "    \n",
    "    # Fit on train\n",
    "    train_df[high_cardinality_cols] = target_encoder.fit_transform(\n",
    "        train_df[high_cardinality_cols], \n",
    "        train_df['is_late']\n",
    "    )\n",
    "    \n",
    "    # Transform test\n",
    "    test_df[high_cardinality_cols] = target_encoder.transform(\n",
    "        test_df[high_cardinality_cols]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTarget encoding applied to {len(high_cardinality_cols)} columns\")\n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Final Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "# Get feature column names (excluding target and metadata)\n",
    "final_exclude = ['is_late', 'delay_days', 'ID'] + [\n",
    "    col for col in train_df.columns if 'Date' in col or col in ['PQ #', 'PO / SO #', 'ASN/DN #', 'delivery_year_month']\n",
    "]\n",
    "\n",
    "feature_columns = [col for col in train_df.columns if col not in final_exclude]\n",
    "\n",
    "# Create feature matrices\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['is_late']\n",
    "\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['is_late']\n",
    "\n",
    "print(f\"Final feature matrix:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nTotal features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for any remaining missing values or infinites\n",
    "print(\"Data quality check:\")\n",
    "print(f\"Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in X_train: {np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Infinite values in X_test: {np.isinf(X_test.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Replace any infinites with NaN, then fill with 0\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(\"\\nAfter cleanup:\")\n",
    "print(f\"Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify numerical columns in final feature matrix\n",
    "numerical_feature_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features to scale: {len(numerical_feature_cols)}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on train, transform both\n",
    "X_train[numerical_feature_cols] = scaler.fit_transform(X_train[numerical_feature_cols])\n",
    "X_test[numerical_feature_cols] = scaler.transform(X_test[numerical_feature_cols])\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler\")\n",
    "print(f\"Mean of scaled features (should be ~0): {X_train[numerical_feature_cols].mean().mean():.6f}\")\n",
    "print(f\"Std of scaled features (should be ~1): {X_train[numerical_feature_cols].std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation with target\n",
    "# Add target back temporarily for correlation analysis\n",
    "X_train_with_target = X_train.copy()\n",
    "X_train_with_target['is_late'] = y_train.values\n",
    "\n",
    "# Compute correlations with target\n",
    "correlations = X_train_with_target.corr()['is_late'].drop('is_late').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Features Correlated with Delay:\")\n",
    "print(\"=\"*60)\n",
    "print(correlations.head(20))\n",
    "\n",
    "print(\"\\nTop 20 Features Negatively Correlated with Delay:\")\n",
    "print(\"=\"*60)\n",
    "print(correlations.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Top 15 positive and negative correlations\n",
    "top_corr = pd.concat([correlations.head(15), correlations.tail(15)])\n",
    "\n",
    "plt.barh(range(len(top_corr)), top_corr.values)\n",
    "plt.yticks(range(len(top_corr)), top_corr.index, fontsize=8)\n",
    "plt.xlabel('Correlation with Delay')\n",
    "plt.title('Top Features Correlated with Delivery Delays')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for highly correlated features (multicollinearity)\n",
    "# Calculate correlation matrix for numerical features only\n",
    "corr_matrix = X_train[numerical_feature_cols].corr().abs()\n",
    "\n",
    "# Find pairs with correlation > 0.9\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nHighly correlated feature pairs (>0.9): {len(high_corr_pairs)}\")\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(\"\\nTop 10 highly correlated pairs:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True)[:10]:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Feature Importance (Simple Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train a simple random forest for feature importance\n",
    "print(\"Training quick Random Forest for feature importance...\")\n",
    "rf_quick = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_quick.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_quick.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Evaluate quick model\n",
    "y_pred_proba = rf_quick.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\nQuick RF Model ROC-AUC: {auc:.4f}\")\n",
    "print(f\"\\nTop 20 Important Features:\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values, fontsize=9)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features by Random Forest Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save train and test sets\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False, header=True)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"Processed data saved:\")\n",
    "print(\"  - ../data/processed/X_train.csv\")\n",
    "print(\"  - ../data/processed/X_test.csv\")\n",
    "print(\"  - ../data/processed/y_train.csv\")\n",
    "print(\"  - ../data/processed/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save feature names for reference\n",
    "feature_names_df = pd.DataFrame({\n",
    "    'feature_name': X_train.columns,\n",
    "    'feature_type': X_train.dtypes.astype(str)\n",
    "})\n",
    "feature_names_df.to_csv('../data/processed/feature_names.csv', index=False)\n",
    "\n",
    "print(\"\\nFeature names saved to: ../data/processed/feature_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY - FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Input Data:\")\n",
    "print(f\"   - Rows: {df.shape[0]:,}\")\n",
    "print(f\"   - Original columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n2. Features Created:\")\n",
    "print(f\"   - Interaction features: 3 (vendor_country, mode_country, vendor_mode)\")\n",
    "print(f\"   - Ratio features: 3 (weight_to_cost, value_density, lead_time_ratio)\")\n",
    "print(f\"   - Boolean features: 4 (high_value, heavy, air, long_lead_time)\")\n",
    "print(f\"   - Time features: 2 (weekend, season)\")\n",
    "\n",
    "print(f\"\\n3. Encoding:\")\n",
    "print(f\"   - One-hot encoded: {len(low_cardinality_cols)} low-cardinality columns\")\n",
    "print(f\"   - Target encoded: {len(high_cardinality_cols)} high-cardinality columns\")\n",
    "\n",
    "print(f\"\\n4. Final Dataset:\")\n",
    "print(f\"   - Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   - Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   - Total features: {X_train.shape[1]}\")\n",
    "print(f\"   - Numerical features scaled: {len(numerical_feature_cols)}\")\n",
    "\n",
    "print(f\"\\n5. Data Quality:\")\n",
    "print(f\"   - Missing values: 0\")\n",
    "print(f\"   - Infinite values: 0\")\n",
    "print(f\"   - Target balance - Train: {y_train.mean():.2%}\")\n",
    "print(f\"   - Target balance - Test: {y_test.mean():.2%}\")\n",
    "\n",
    "print(f\"\\n6. Quick Model Performance:\")\n",
    "print(f\"   - Random Forest ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n7. Top 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n8. Next Steps:\")\n",
    "print(\"   - Notebook 4: Baseline Models (Logistic Regression, Decision Tree)\")\n",
    "print(\"   - Notebook 5: Advanced Models (Random Forest, XGBoost, LightGBM)\")\n",
    "print(\"   - Notebook 6: Model Evaluation and Selection\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report,\n    roc_curve, precision_recall_curve,\n    roc_auc_score\n)\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12,6)\n\nprint(\"Libraries imported successfully!\")\nprint(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model comparison results\n",
    "model_comparison = pd.read_csv('../models/model_metadata/model_comparison.csv')\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*100)\n",
    "print(model_comparison[['Model', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC']].to_string(index=False))\n",
    "\n",
    "# Load all predictions\n",
    "predictions = pd.read_csv('../models/model_metadata/all_predictions.csv')\n",
    "print(f\"\\nPredictions loaded: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data for analysis\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "\n",
    "print(f\"Test data loaded: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detailed Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify best model by different criteria\n",
    "print(\"Best Models by Metric:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    best_model = model_comparison.loc[model_comparison[metric].idxmax(), 'Model']\n",
    "    best_score = model_comparison[metric].max()\n",
    "    print(f\"{name:12s}: {best_model:20s} ({best_score:.4f})\")\n",
    "\n",
    "# Overall recommendation based on ROC-AUC\n",
    "recommended_model = model_comparison.loc[model_comparison['Test_ROC_AUC'].idxmax(), 'Model']\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Recommended Model (based on ROC-AUC): {recommended_model}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Performance improvement analysis\n",
    "baseline_auc = model_comparison[model_comparison['Model'].isin(['Logistic Regression', 'Decision Tree'])]['Test_ROC_AUC'].max()\n",
    "best_auc = model_comparison['Test_ROC_AUC'].max()\n",
    "improvement = ((best_auc - baseline_auc) / baseline_auc) * 100\n",
    "\n",
    "print(f\"\\nPerformance Improvement:\")\n",
    "print(f\"  Baseline (best simple model): {baseline_auc:.4f}\")\n",
    "print(f\"  Best model: {best_auc:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ROI calculation (compare to no model scenario)\n# Assume without model, we catch 0% of delays\ntotal_delays = predictions['y_true'].sum()\ncost_without_model = total_delays * COST_FALSE_NEGATIVE\n\nprint(\"ROI Analysis:\")\nprint(\"=\"*80)\nprint(f\"Total delays in test set: {total_delays}\")\nprint(f\"Cost without prediction model: ${cost_without_model:,.0f}\")\nprint(f\"\\\\nCost savings with {recommended_model}:\")\n\nmodel_cost = business_df[business_df['Model'] == recommended_model]['Total_Cost'].values[0]\nsavings = cost_without_model - model_cost\nroi = (savings / cost_without_model) * 100\n\n\nprint(f\"  Model cost: ${model_cost:,.0f}\")\nprint(f\"  Savings: ${savings:,.0f}\")\nprint(f\"  ROI: {roi:.1f}%\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create error analysis dataframe\n",
    "error_analysis = X_test.copy()\n",
    "error_analysis['y_true'] = predictions['y_true'].values\n",
    "error_analysis['y_pred'] = predictions[best_pred_col].values\n",
    "error_analysis['y_proba'] = predictions[best_proba_col].values\n",
    "\n",
    "# Classify errors\n",
    "error_analysis['prediction_type'] = 'Correct'\n",
    "error_analysis.loc[(error_analysis['y_true'] == 1) & (error_analysis['y_pred'] == 0), 'prediction_type'] = 'False Negative'\n",
    "error_analysis.loc[(error_analysis['y_true'] == 0) & (error_analysis['y_pred'] == 1), 'prediction_type'] = 'False Positive'\n",
    "\n",
    "print(\"\\nPrediction Type Distribution:\")\n",
    "print(error_analysis['prediction_type'].value_counts())\n",
    "print(f\"\\nError rate: {(error_analysis['prediction_type'] != 'Correct').mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detailed confusion matrix with labels\n",
    "cm = confusion_matrix(predictions['y_true'], predictions[best_pred_col])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['On-Time', 'Late'],\n",
    "            yticklabels=['On-Time', 'Late'])\n",
    "plt.title(f'Confusion Matrix - {recommended_model}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Add percentages\n",
    "total = cm.sum()\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j+0.5, i+0.7, f'({cm[i,j]/total*100:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(f\"\\nClassification Report - {recommended_model}:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(predictions['y_true'], predictions[best_pred_col], \n",
    "                          target_names=['On-Time', 'Late']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze false negatives (missed delays)\n",
    "false_negatives = error_analysis[error_analysis['prediction_type'] == 'False Negative']\n",
    "false_positives = error_analysis[error_analysis['prediction_type'] == 'False Positive']\n",
    "\n",
    "print(f\"\\nFalse Negative Analysis (Missed Delays):\")\n",
    "print(f\"  Count: {len(false_negatives)}\")\n",
    "print(f\"  Avg prediction probability: {false_negatives['y_proba'].mean():.3f}\")\n",
    "print(f\"  Probability range: {false_negatives['y_proba'].min():.3f} - {false_negatives['y_proba'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nFalse Positive Analysis (False Alarms):\")\n",
    "print(f\"  Count: {len(false_positives)}\")\n",
    "print(f\"  Avg prediction probability: {false_positives['y_proba'].mean():.3f}\")\n",
    "print(f\"  Probability range: {false_positives['y_proba'].min():.3f} - {false_positives['y_proba'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define business costs (from config)\n",
    "COST_FALSE_NEGATIVE = 5000  # Cost of missing a delay (expedited shipping, penalties)\n",
    "COST_FALSE_POSITIVE = 500   # Cost of false alarm (staff time, unnecessary intervention)\n",
    "\n",
    "# Calculate business costs for each model\n",
    "business_costs = []\n",
    "\n",
    "for model_name in model_comparison['Model']:\n",
    "    pred_col = model_pred_map[model_name]\n",
    "    \n",
    "    fn = ((predictions['y_true'] == 1) & (predictions[pred_col] == 0)).sum()\n",
    "    fp = ((predictions['y_true'] == 0) & (predictions[pred_col] == 1)).sum()\n",
    "    \n",
    "    total_cost = (fn * COST_FALSE_NEGATIVE) + (fp * COST_FALSE_POSITIVE)\n",
    "    \n",
    "    business_costs.append({\n",
    "        'Model': model_name,\n",
    "        'False_Negatives': fn,\n",
    "        'False_Positives': fp,\n",
    "        'FN_Cost': fn * COST_FALSE_NEGATIVE,\n",
    "        'FP_Cost': fp * COST_FALSE_POSITIVE,\n",
    "        'Total_Cost': total_cost\n",
    "    })\n",
    "\n",
    "business_df = pd.DataFrame(business_costs).sort_values('Total_Cost')\n",
    "\n",
    "print(\"Business Cost Analysis:\")\n",
    "print(\"=\"*100)\n",
    "print(business_df.to_string(index=False))\n",
    "\n",
    "# Best model by business cost\n",
    "best_business_model = business_df.iloc[0]['Model']\n",
    "best_business_cost = business_df.iloc[0]['Total_Cost']\n",
    "print(f\"\\nBest Model by Business Cost: {best_business_model} (${best_business_cost:,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize business costs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total cost comparison\n",
    "axes[0].barh(business_df['Model'], business_df['Total_Cost']/1000)\n",
    "axes[0].set_xlabel('Total Cost ($1000s)')\n",
    "axes[0].set_title('Total Business Cost by Model')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cost breakdown\n",
    "x = np.arange(len(business_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, business_df['FN_Cost']/1000, width, label='False Negative Cost')\n",
    "axes[1].bar(x + width/2, business_df['FP_Cost']/1000, width, label='False Positive Cost')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Cost ($1000s)')\n",
    "axes[1].set_title('Cost Breakdown by Error Type')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(business_df['Model'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_metadata/business_cost_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROI calculation (compare to no model scenario)\n",
    "# Assume without model, we catch 0% of delays\n",
    "total_delays = predictions['y_true'].sum()\n",
    "cost_without_model = total_delays * COST_FALSE_NEGATIVE\n",
    "\n",
    "print(\"ROI Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total delays in test set: {total_delays}\")\n",
    "print(f\"Cost without prediction model: ${cost_without_model:,.0f}\")\n",
    "print(f\"\\nCost savings with {recommended_model}:\")\n",
    "\n",
    "model_cost = business_df[business_df['Model'] == recommended_model]['Total_Cost'].values[0]\n",
    "savings = cost_without_model - model_cost\n",
    "roi = (savings / cost_without_model) * 100\n",
    "\n",
    "print(f\"  Model cost: ${model_cost:,.0f}\")\n",
    "print(f\"  Savings: ${savings:,.0f}\")\n",
    "print(f\"  ROI: {roi:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze different thresholds for the best model\n",
    "thresholds_to_test = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_thresh = (predictions[best_proba_col] >= threshold).astype(int)\n",
    "    \n",
    "    fn = ((predictions['y_true'] == 1) & (y_pred_thresh == 0)).sum()\n",
    "    fp = ((predictions['y_true'] == 0) & (y_pred_thresh == 1)).sum()\n",
    "    tp = ((predictions['y_true'] == 1) & (y_pred_thresh == 1)).sum()\n",
    "    tn = ((predictions['y_true'] == 0) & (y_pred_thresh == 0)).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    total_cost = (fn * COST_FALSE_NEGATIVE) + (fp * COST_FALSE_POSITIVE)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'Total_Cost': total_cost,\n",
    "        'FN': fn,\n",
    "        'FP': fp\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find optimal threshold by minimum cost\n",
    "optimal_threshold = threshold_df.loc[threshold_df['Total_Cost'].idxmin(), 'Threshold']\n",
    "optimal_cost = threshold_df['Total_Cost'].min()\n",
    "\n",
    "print(f\"Threshold Optimization Results:\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"  Optimal cost: ${optimal_cost:,.0f}\")\n",
    "print(f\"  Default threshold (0.5) cost: ${threshold_df[threshold_df['Threshold']==0.5]['Total_Cost'].values[0]:,.0f}\")\n",
    "print(f\"  Cost improvement: ${threshold_df[threshold_df['Threshold']==0.5]['Total_Cost'].values[0] - optimal_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Precision-Recall tradeoff\n",
    "axes[0, 0].plot(threshold_df['Threshold'], threshold_df['Precision'], label='Precision', marker='o')\n",
    "axes[0, 0].plot(threshold_df['Threshold'], threshold_df['Recall'], label='Recall', marker='s')\n",
    "axes[0, 0].plot(threshold_df['Threshold'], threshold_df['F1'], label='F1 Score', marker='^')\n",
    "axes[0, 0].axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "axes[0, 0].set_xlabel('Threshold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Precision-Recall-F1 vs Threshold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total cost vs threshold\n",
    "axes[0, 1].plot(threshold_df['Threshold'], threshold_df['Total_Cost']/1000, marker='o', color='red')\n",
    "axes[0, 1].axvline(x=optimal_threshold, color='g', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "axes[0, 1].set_xlabel('Threshold')\n",
    "axes[0, 1].set_ylabel('Total Cost ($1000s)')\n",
    "axes[0, 1].set_title('Business Cost vs Threshold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# False negatives vs false positives\n",
    "axes[1, 0].plot(threshold_df['Threshold'], threshold_df['FN'], label='False Negatives', marker='o')\n",
    "axes[1, 0].plot(threshold_df['Threshold'], threshold_df['FP'], label='False Positives', marker='s')\n",
    "axes[1, 0].axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "axes[1, 0].set_xlabel('Threshold')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Error Counts vs Threshold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(predictions['y_true'], predictions[best_proba_col])\n",
    "axes[1, 1].plot(recall_curve, precision_curve, linewidth=2)\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Precision-Recall Curve')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_metadata/threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Selection & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*100)\n",
    "print(\"FINAL MODEL SELECTION & DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n1. RECOMMENDED MODEL: {recommended_model}\")\n",
    "print(f\"   {'='*80}\")\n",
    "\n",
    "# Get metrics for recommended model\n",
    "rec_metrics = model_comparison[model_comparison['Model'] == recommended_model].iloc[0]\n",
    "rec_business = business_df[business_df['Model'] == recommended_model].iloc[0]\n",
    "\n",
    "print(f\"\\n   Performance Metrics:\")\n",
    "print(f\"     - Test Accuracy:  {rec_metrics['Test_Accuracy']:.4f}\")\n",
    "print(f\"     - Test Precision: {rec_metrics['Test_Precision']:.4f}\")\n",
    "print(f\"     - Test Recall:    {rec_metrics['Test_Recall']:.4f}\")\n",
    "print(f\"     - Test F1 Score:  {rec_metrics['Test_F1']:.4f}\")\n",
    "print(f\"     - Test ROC-AUC:   {rec_metrics['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Business Metrics:\")\n",
    "print(f\"     - False Negatives: {rec_business['False_Negatives']}\")\n",
    "print(f\"     - False Positives: {rec_business['False_Positives']}\")\n",
    "print(f\"     - Total Cost: ${rec_business['Total_Cost']:,.0f}\")\n",
    "print(f\"     - Cost Savings vs No Model: ${savings:,.0f} ({roi:.1f}% ROI)\")\n",
    "\n",
    "print(f\"\\n   Recommended Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"     - Optimized for minimum business cost\")\n",
    "print(f\"     - Additional savings: ${threshold_df[threshold_df['Threshold']==0.5]['Total_Cost'].values[0] - optimal_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f\"\\n2. WHY THIS MODEL?\")\n",
    "print(f\"   {'='*80}\")\n",
    "print(f\"   - Highest ROC-AUC score: {rec_metrics['Test_ROC_AUC']:.4f}\")\n",
    "print(f\"   - Balanced precision and recall\")\n",
    "print(f\"   - Strong performance across all metrics\")\n",
    "print(f\"   - Reasonable computational cost\")\n",
    "print(f\"   - Good generalization (low train-test gap)\")\n",
    "\n",
    "train_test_gap = rec_metrics['Train_ROC_AUC'] - rec_metrics['Test_ROC_AUC']\n",
    "print(f\"   - Train-Test ROC-AUC gap: {train_test_gap:.4f} (low overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f\"\\n3. DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(f\"   {'='*80}\")\n",
    "print(f\"\\n   Model Configuration:\")\n",
    "print(f\"     - Model: {recommended_model}\")\n",
    "print(f\"     - Decision Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"     - Model File: ../models/saved_models/{recommended_model.lower().replace(' ', '_')}_model.pkl\")\n",
    "\n",
    "print(f\"\\n   Implementation Strategy:\")\n",
    "print(f\"     1. Deploy model with optimal threshold ({optimal_threshold:.2f})\")\n",
    "print(f\"     2. Monitor false negative rate (target: <{rec_business['False_Negatives']/len(y_test):.1%})\")\n",
    "print(f\"     3. Alert operations team when prediction probability > {optimal_threshold:.2f}\")\n",
    "print(f\"     4. Implement intervention workflow for predicted delays\")\n",
    "\n",
    "print(f\"\\n   Monitoring Requirements:\")\n",
    "print(f\"     - Track model performance weekly\")\n",
    "print(f\"     - Monitor data drift in key features\")\n",
    "print(f\"     - Retrain model quarterly or when AUC drops below {rec_metrics['Test_ROC_AUC']-0.05:.2f}\")\n",
    "print(f\"     - A/B test threshold adjustments\")\n",
    "\n",
    "print(f\"\\n   Business Impact:\")\n",
    "print(f\"     - Expected annual savings: ~${(savings * 52):,.0f} (assuming weekly batches)\")\n",
    "print(f\"     - Reduced late deliveries by catching {rec_metrics['Test_Recall']:.0%} of delays\")\n",
    "print(f\"     - Improved customer satisfaction through proactive communication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f\"\\n4. NEXT STEPS FOR PRODUCTION\")\n",
    "print(f\"   {'='*80}\")\n",
    "print(f\"\\n   Technical:\")\n",
    "print(f\"     □ Create prediction API/service\")\n",
    "print(f\"     □ Implement feature pipeline for new data\")\n",
    "print(f\"     □ Set up model versioning and rollback\")\n",
    "print(f\"     □ Build monitoring dashboard\")\n",
    "print(f\"     □ Implement automated retraining pipeline\")\n",
    "\n",
    "print(f\"\\n   Business:\")\n",
    "print(f\"     □ Define intervention protocols for predicted delays\")\n",
    "print(f\"     □ Train operations team on model outputs\")\n",
    "print(f\"     □ Set up escalation workflows\")\n",
    "print(f\"     □ Measure actual cost savings post-deployment\")\n",
    "print(f\"     □ Gather feedback for model improvements\")\n",
    "\n",
    "print(f\"\\n   Data Science:\")\n",
    "print(f\"     □ Experiment with additional features\")\n",
    "print(f\"     □ Try ensemble of top models\")\n",
    "print(f\"     □ Implement SHAP for detailed explainability\")\n",
    "print(f\"     □ Analyze model performance by segment\")\n",
    "print(f\"     □ Build separate models for different routes/products\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create final report summary\n",
    "final_report = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'recommended_model': recommended_model,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'test_roc_auc': rec_metrics['Test_ROC_AUC'],\n",
    "    'test_precision': rec_metrics['Test_Precision'],\n",
    "    'test_recall': rec_metrics['Test_Recall'],\n",
    "    'test_f1': rec_metrics['Test_F1'],\n",
    "    'false_negatives': int(rec_business['False_Negatives']),\n",
    "    'false_positives': int(rec_business['False_Positives']),\n",
    "    'total_business_cost': float(rec_business['Total_Cost']),\n",
    "    'estimated_savings': float(savings),\n",
    "    'roi_percentage': float(roi)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/model_metadata/final_model_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(\"Final report saved to: ../models/model_metadata/final_model_report.json\")\n",
    "print(\"\\nProject completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
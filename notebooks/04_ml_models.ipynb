{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport time\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Evaluation metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, precision_recall_curve\n)\nfrom sklearn.model_selection import cross_val_score\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12,6)\n\nprint(\"Libraries imported successfully!\")\nprint(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load processed data\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Train - Delay rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test - Delay rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Probabilities\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train_Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Test_Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'Train_Precision': precision_score(y_train, y_train_pred),\n",
    "        'Test_Precision': precision_score(y_test, y_test_pred),\n",
    "        'Train_Recall': recall_score(y_train, y_train_pred),\n",
    "        'Test_Recall': recall_score(y_test, y_test_pred),\n",
    "        'Train_F1': f1_score(y_train, y_train_pred),\n",
    "        'Test_F1': f1_score(y_test, y_test_pred),\n",
    "        'Train_ROC_AUC': roc_auc_score(y_train, y_train_proba),\n",
    "        'Test_ROC_AUC': roc_auc_score(y_test, y_test_proba)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_test_pred, y_test_proba\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Print model evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {metrics['Model']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nTrain Metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['Train_Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['Train_Precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['Train_Recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['Train_F1']:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {metrics['Train_ROC_AUC']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['Test_Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['Test_Precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['Test_Recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['Test_F1']:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {metrics['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Feature importance from Decision Tree\nfeature_importance_dt = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': dt_model.feature_importances_\n}).sort_values('importance', ascending=False).head(20)\n\nprint(\"\\\\nTop 20 Important Features (Decision Tree):\")\nprint(feature_importance_dt)\n\n# Visualze (minor typo)\nplt.figure(figsize=(10, 8))\nplt.barh(range(len(feature_importance_dt)), feature_importance_dt['importance'].values)\nplt.yticks(range(len(feature_importance_dt)), feature_importance_dt['feature'].values, fontsize=9)\nplt.xlabel('Feature Importance')\nplt.title('Top 20 Features - Decision Tree')\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['On-Time', 'Late'],\n",
    "            yticklabels=['On-Time', 'Late'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_pred, target_names=['On-Time', 'Late']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Decision Tree\n",
    "print(\"Training Decision Tree...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "dt_metrics, dt_pred, dt_proba = evaluate_model(dt_model, X_train, y_train, X_test, y_test, 'Decision Tree')\n",
    "print_metrics(dt_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from Decision Tree\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 Important Features (Decision Tree):\")\n",
    "print(feature_importance_dt)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(feature_importance_dt)), feature_importance_dt['importance'].values)\n",
    "plt.yticks(range(len(feature_importance_dt)), feature_importance_dt['feature'].values, fontsize=9)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics, rf_pred, rf_proba = evaluate_model(rf_model, X_train, y_train, X_test, y_test, 'Random Forest')\n",
    "print_metrics(rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 Important Features (Random Forest):\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(feature_importance_rf)), feature_importance_rf['importance'].values)\n",
    "plt.yticks(range(len(feature_importance_rf)), feature_importance_rf['feature'].values, fontsize=9)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics, xgb_pred, xgb_proba = evaluate_model(xgb_model, X_train, y_train, X_test, y_test, 'XGBoost')\n",
    "print_metrics(xgb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from XGBoost\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 Important Features (XGBoost):\")\n",
    "print(feature_importance_xgb)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(feature_importance_xgb)), feature_importance_xgb['importance'].values)\n",
    "plt.yticks(range(len(feature_importance_xgb)), feature_importance_xgb['feature'].values, fontsize=9)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Model 3: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "lgbm_metrics, lgbm_pred, lgbm_proba = evaluate_model(lgbm_model, X_train, y_train, X_test, y_test, 'LightGBM')\n",
    "print_metrics(lgbm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from LightGBM\n",
    "feature_importance_lgbm = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgbm_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 Important Features (LightGBM):\")\n",
    "print(feature_importance_lgbm)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(feature_importance_lgbm)), feature_importance_lgbm['importance'].values)\n",
    "plt.yticks(range(len(feature_importance_lgbm)), feature_importance_lgbm['feature'].values, fontsize=9)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile all metrics\n",
    "all_metrics = pd.DataFrame([\n",
    "    lr_metrics,\n",
    "    dt_metrics,\n",
    "    rf_metrics,\n",
    "    xgb_metrics,\n",
    "    lgbm_metrics\n",
    "])\n",
    "\n",
    "print(\"\\nModel Comparison - Test Set Performance:\")\n",
    "print(\"=\"*100)\n",
    "print(all_metrics[['Model', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC']].to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "all_metrics.to_csv('../models/model_metadata/model_comparison.csv', index=False)\n",
    "print(\"\\nModel comparison saved to: ../models/model_metadata/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics_to_plot = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_ROC_AUC']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'ROC-AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    axes[row, col].bar(all_metrics['Model'], all_metrics[metric])\n",
    "    axes[row, col].set_title(f'Test {title} Comparison')\n",
    "    axes[row, col].set_ylabel(title)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(all_metrics[metric]):\n",
    "        axes[row, col].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_metadata/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Curves Comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "models_data = [\n",
    "    ('Logistic Regression', lr_proba),\n",
    "    ('Decision Tree', dt_proba),\n",
    "    ('Random Forest', rf_proba),\n",
    "    ('XGBoost', xgb_proba),\n",
    "    ('LightGBM', lgbm_proba)\n",
    "]\n",
    "\n",
    "for model_name, y_proba in models_data:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - All Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_metadata/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save all models\n",
    "models_to_save = {\n",
    "    'logistic_regression': lr_model,\n",
    "    'decision_tree': dt_model,\n",
    "    'random_forest': rf_model,\n",
    "    'xgboost': xgb_model,\n",
    "    'lightgbm': lgbm_model\n",
    "}\n",
    "\n",
    "for model_name, model in models_to_save.items():\n",
    "    filename = f'../models/saved_models/{model_name}_model.pkl'\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save predictions for later analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'lr_pred': lr_pred,\n",
    "    'lr_proba': lr_proba,\n",
    "    'dt_pred': dt_pred,\n",
    "    'dt_proba': dt_proba,\n",
    "    'rf_pred': rf_pred,\n",
    "    'rf_proba': rf_proba,\n",
    "    'xgb_pred': xgb_pred,\n",
    "    'xgb_proba': xgb_proba,\n",
    "    'lgbm_pred': lgbm_pred,\n",
    "    'lgbm_proba': lgbm_proba\n",
    "})\n",
    "\n",
    "predictions_df.to_csv('../models/model_metadata/all_predictions.csv', index=False)\n",
    "print(\"Predictions saved to: ../models/model_metadata/all_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*100)\n",
    "print(\"SUMMARY - ML MODELS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. Models Trained:\")\n",
    "print(\"   Baseline Models:\")\n",
    "print(\"   - Logistic Regression\")\n",
    "print(\"   - Decision Tree\")\n",
    "print(\"   \\n   Advanced Models:\")\n",
    "print(\"   - Random Forest\")\n",
    "print(\"   - XGBoost\")\n",
    "print(\"   - LightGBM\")\n",
    "\n",
    "print(\"\\n2. Best Model by Metric:\")\n",
    "print(f\"   - Best Accuracy:  {all_metrics.loc[all_metrics['Test_Accuracy'].idxmax(), 'Model']} ({all_metrics['Test_Accuracy'].max():.4f})\")\n",
    "print(f\"   - Best Precision: {all_metrics.loc[all_metrics['Test_Precision'].idxmax(), 'Model']} ({all_metrics['Test_Precision'].max():.4f})\")\n",
    "print(f\"   - Best Recall:    {all_metrics.loc[all_metrics['Test_Recall'].idxmax(), 'Model']} ({all_metrics['Test_Recall'].max():.4f})\")\n",
    "print(f\"   - Best F1 Score:  {all_metrics.loc[all_metrics['Test_F1'].idxmax(), 'Model']} ({all_metrics['Test_F1'].max():.4f})\")\n",
    "print(f\"   - Best ROC-AUC:   {all_metrics.loc[all_metrics['Test_ROC_AUC'].idxmax(), 'Model']} ({all_metrics['Test_ROC_AUC'].max():.4f})\")\n",
    "\n",
    "print(\"\\n3. Model Performance Range:\")\n",
    "print(f\"   - Test Accuracy:  {all_metrics['Test_Accuracy'].min():.4f} - {all_metrics['Test_Accuracy'].max():.4f}\")\n",
    "print(f\"   - Test ROC-AUC:   {all_metrics['Test_ROC_AUC'].min():.4f} - {all_metrics['Test_ROC_AUC'].max():.4f}\")\n",
    "\n",
    "print(\"\\n4. Saved Artifacts:\")\n",
    "print(\"   - 5 trained models (.pkl files)\")\n",
    "print(\"   - Model comparison CSV\")\n",
    "print(\"   - All predictions CSV\")\n",
    "print(\"   - Comparison plots (PNG)\")\n",
    "\n",
    "print(\"\\n5. Next Steps:\")\n",
    "print(\"   - Notebook 5: Model Evaluation & Selection\")\n",
    "print(\"   - Error analysis and SHAP values\")\n",
    "print(\"   - Final model selection\")\n",
    "print(\"   - Deployment recommendations\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}